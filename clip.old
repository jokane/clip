import dis
from enum import Enum
import glob
import inspect
import io
import zipfile
import sys

import numba
import pdf2image
import scipy.signal

class Clip(ABC):

    def preview(self):
        """Render the video part and display it in a window on screen."""
        with custom_progressbar("Previewing", self.num_frames()) as pb:
            for i in range(0, self.num_frames()):
                frame = self.get_frame_cached(i)
                pb.update(i)
                cv2.imshow("", frame)
                cv2.waitKey(1)

        cv2.destroyWindow("")

    def save_play_quit(self, filename="spq.mp4"): # pragma: no cover
        """ Save the video, play it, and then end the process.  Useful
        sometimes when debugging to see a particular clip without running the
        entire program. """
        self.save(filename)
        os.system("mplayer " + filename)
        sys.exit(0)

    spq = save_play_quit



def join(video_clip, audio_clip):
    """ Create a new clip that combines the video of one clip with the audio of
    another.  The length will be the length of the longer of the two."""
    require_clip(video_clip, "video clip")
    require_clip(audio_clip, "audio clip")

    assert not isinstance(video_clip, AudioClip)
    assert not isinstance(audio_clip, VideoClip)
    assert not isinstance(audio_clip, solid)

    return composite(Element(video_clip, 0, [0,0],
                             video_mode=VideoMode.REPLACE,
                             audio_mode=AudioMode.IGNORE),
                     Element(audio_clip, 0, [0,0],
                             video_mode=VideoMode.IGNORE,
                             audio_mode=AudioMode.REPLACE))

@numba.jit(nopython=True) # pragma: no cover
def alpha_blend(f0, f1):
    """ Blend two equally-sized RGBA images and return the result. """
    # https://stackoverflow.com/questions/28900598/how-to-combine-two-colors-with-varying-alpha-values
    # assert f0.shape == f1.shape, f'{f0.shape}!={f1.shape}'
    # assert f0.dtype == np.uint8
    # assert f1.dtype == np.uint8

    f0 = f0.astype(np.float64) / 255.0
    f1 = f1.astype(np.float64) / 255.0

    b0 = f0[:,:,0]
    g0 = f0[:,:,1]
    r0 = f0[:,:,2]
    a0 = f0[:,:,3]

    b1 = f1[:,:,0]
    g1 = f1[:,:,1]
    r1 = f1[:,:,2]
    a1 = f1[:,:,3]

    a01 = (1 - a0)*a1 + a0
    b01 = (1 - a0)*b1 + a0*b0
    g01 = (1 - a0)*g1 + a0*g0
    r01 = (1 - a0)*r1 + a0*r0

    f01 = np.zeros(shape=f0.shape, dtype=np.float64)

    f01[:,:,0] = b01
    f01[:,:,1] = g01
    f01[:,:,2] = r01
    f01[:,:,3] = a01
    f01 = (f01*255.0).astype(np.uint8)

    return f01

class VideoMode(Enum):
    """ When defining an element of a composite, how should the video for this
    element be composited into the final clip?"""
    REPLACE = 1
    BLEND = 2
    ADD = 3
    IGNORE = 4

class AudioMode(Enum):
    """ When defining and element of a composite, how should the video for this
    element be composited into the final clip?"""
    REPLACE = 5
    ADD = 6
    IGNORE = 7

class Element:
    """An element to be included in a composite."""

    def __init__(self, clip, start_time, position, video_mode=VideoMode.REPLACE,
                 audio_mode=AudioMode.REPLACE):
        require_clip(clip, "clip")
        require_float(start_time, "start_time")
        require_non_negative(start_time, "start_time")

        if is_iterable(position):
            if len(position) != 2:
                raise ValueError(f'Position should be tuple (x,y) or callable.  '
                                 f'Got {type(position)} {position} instead.')
            require_int(position[0], "position x")
            require_int(position[1], "position y")

        elif not callable(position):
            raise TypeError(f'Position should be tuple (x,y) or callable,'
                            f'not {type(position)} {position}')

        if not isinstance(video_mode, VideoMode):
            raise TypeError(f'Video mode cannot be {video_mode}.')

        if not isinstance(audio_mode, AudioMode):
            raise TypeError(f'Audio mode cannot be {audio_mode}.')

        self.clip = clip
        self.start_time = start_time
        self.position = position
        self.video_mode = video_mode
        self.audio_mode = audio_mode

    def start_index(self):
        """ Return the index at which this element begins. """
        return int(self.start_time*self.clip.frame_rate())

    def required_dimensions(self):
        """ Return the (width, height) needed to show this element as fully as
        possible.  (May not be all of the clip, because the top left is always
        (0,0), so things at negative coordinates will still be hidden.) """
        if callable(self.position):
            nw, nh = 0, 0
            for index in range(self.clip.num_frames()):
                pos = self.position(index)
                nw = max(nw, pos[0] + self.clip.width())
                nh = max(nh, pos[1] + self.clip.height())
            return (nw, nh)
        else:
            return (self.position[0] + self.clip.width(),
                    self.position[1] + self.clip.height())

    def signature(self, index):
        """ A signature for this element, to be used to create the overall
        frame signature.  Returns None if this element does not contribute to
        this frame. """
        start_index = self.start_index()
        if self.video_mode==VideoMode.IGNORE:
            return None
        if index < start_index or index >= start_index + self.clip.num_frames():
            return None
        clip_index = index - self.start_index()
        assert clip_index >= 0
        if callable(self.position):
            pos = self.position(index - start_index)
        else:
            pos = self.position
        return [self.video_mode, pos, self.clip.frame_signature(clip_index)]

    def get_coordinates(self, index, shape):
        """ Compute the coordinates at which this element should appear at the
        given index. """
        if callable(self.position):
            pos = self.position(index)
        else:
            pos = self.position
        x = pos[0]
        y = pos[1]
        x0 = x
        x1 = x + shape[1]
        y0 = y
        y1 = y + shape[0]
        return x0, x1, y0, y1

    def apply_to_frame(self, under, index):
        """ Modify the given frame as described by this element. """
        # If this element does not apply at this index, make no change.
        start_index = self.start_index()
        clip_index = index - start_index
        if index < start_index or index >= start_index + self.clip.num_frames():
            return

        # Get the frame that we're compositing in and figure out where it goes.
        over_patch = self.clip.get_frame(clip_index)
        x0, x1, y0, y1 = self.get_coordinates(clip_index, over_patch.shape)

        # If it's totally off-screen, make no change.
        if x1 < 0 or x0 > under.shape[1] or y1 < 0 or y0 > under.shape[0]:
            return

        # Clip the frame itself if needed to fit.
        if x0 < 0:
            over_patch = over_patch[:,-x0:,:]
            x0 = 0
        if x1 >= under.shape[1]:
            over_patch = over_patch[:,0:under.shape[1]-x0,:]
            x1 = under.shape[1]

        if y0 < 0:
            over_patch = over_patch[-y0:,:,:]
            y0 = 0
        if y1 >= under.shape[0]:
            over_patch = over_patch[0:under.shape[0]-y0,:,:]
            y1 = under.shape[0]

        # Actually do the compositing, based on the video mode.
        if self.video_mode == VideoMode.REPLACE:
            under[y0:y1, x0:x1, :] = over_patch
        elif self.video_mode == VideoMode.BLEND:
            under_patch = under[y0:y1, x0:x1, :]
            blended = alpha_blend(over_patch, under_patch)
            under[y0:y1, x0:x1, :] = blended
        elif self.video_mode == VideoMode.ADD:
            under[y0:y1, x0:x1, :] += over_patch
        elif self.video_mode == VideoMode.IGNORE:
            pass
        else:
            raise NotImplementedError(self.video_mode) # pragma: no cover

class composite(Clip):
    """ Given a collection of elements, form a composite clip."""
    def __init__(self, *args, width=None, height=None, length=None):
        super().__init__()

        self.elements = flatten_args(args)

        # Sanity check on the inputs.
        for (i, e) in enumerate(self.elements):
            assert isinstance(e, Element)
            require_non_negative(e.start_time, f'start time {i}')
        if width is not None:
            require_int(width, "width")
            require_positive(width, "width")
        if height is not None:
            require_int(height, "height")
            require_positive(height, "height")
        if length is not None:
            require_float(length, "length")
            require_positive(length, "length")

        # Check for mismatches in the rates.
        e0 = self.elements[0]
        for (i, e) in enumerate(self.elements[1:]):
            require_equal(e0.clip.frame_rate(), e.clip.frame_rate(), "frame rates")
            require_equal(e0.clip.sample_rate(), e.clip.sample_rate(), "sample rates")

        # Compute the width, height, and length of the result.  If we're
        # given any of these, use that.  Otherwise, make it big enough for
        # every element to fit.
        if width is None or height is None:
            nw, nh = 0, 0
            for e in self.elements:
                if e.video_mode != VideoMode.IGNORE:
                    dim = e.required_dimensions()
                    nw = max(nw, dim[0])
                    nh = max(nh, dim[1])
            if width is None:
                width = nw
            if height is None:
                height = nh

        if length is None:
            length = max(map(lambda e: e.start_time + e.clip.length(), self.elements))

        self.metrics = Metrics(
          src=e0.clip.metrics,
          width=width,
          height=height,
          length=length
        )


    def frame_signature(self, index):
        sig = ['composite']
        for e in self.elements:
            esig = e.signature(index)
            if esig is not None:
                sig.append(esig)
        return sig

    def get_frame(self, index):
        frame = np.zeros([self.metrics.height, self.metrics.width, 4], np.uint8)
        for e in self.elements:
            e.apply_to_frame(frame, index)
        return frame

    def get_samples(self):
        samples = np.zeros([self.metrics.num_samples(), self.metrics.num_channels])
        for e in self.elements:
            clip_samples = e.clip.get_samples()
            start_sample = int(e.start_time*e.clip.sample_rate())
            end_sample = start_sample + e.clip.num_samples()
            if end_sample > self.num_samples():
                end_sample = self.num_samples()
                clip_samples = clip_samples[0:self.num_samples()-start_sample]

            if e.audio_mode == AudioMode.REPLACE:
                samples[start_sample:end_sample] = clip_samples
            elif e.audio_mode == AudioMode.ADD:
                samples[start_sample:end_sample] += clip_samples
            elif e.audio_mode == AudioMode.IGNORE:
                pass
            else:
                raise NotImplementedError(e.audio_mode) # pragma: no cover

        return samples

def chain(*args, length=None, fade_time = 0):
    """ Concatenate a series of clips.  The clips may be given individually, in
    lists or other iterables, or a mixture of both.  Optionally overlap them a
    little and fade between them."""
    # Construct our list of clips.  Flatten each list; keep each individual
    # clip.
    clips = flatten_args(args)

    # Sanity checks.
    require_float(fade_time, "fade time")
    require_non_negative(fade_time, "fade time")

    for clip in clips:
        require_clip(clip, "clip")

    if len(clips) == 0:
        raise ValueError("Need at least one clip to form a chain.")

    # Figure out when each clip should start and make a list of elements for
    # composite.
    start_time = 0
    elements = []
    for i, clip in enumerate(clips):
        if fade_time>0:
            if i>0:
                clip = fade_in(clip, fade_time)
            if i<len(clips)-1:
                clip = fade_out(clip, fade_time)
            vmode = VideoMode.ADD
        else:
            vmode = VideoMode.REPLACE

        elements.append(Element(clip=clip,
                                start_time=start_time,
                                position=(0,0),
                                video_mode=vmode,
                                audio_mode=AudioMode.ADD))

        start_time += clip.length() - fade_time

    # Let composite do all the work.
    return composite(*elements, length=length)



class from_file(Clip):


class slice_clip(MutatorClip):
    """ Extract the portion of a clip between the given times. Endpoints
    default to the start and end of the clip."""
    def __init__(self, clip, start=0, end=None):
        super().__init__(clip)
        if end is None:
            end = self.clip.length()

        require_float(start, "start time")
        require_non_negative(start, "start time")
        require_float(end, "end time")
        require_non_negative(end, "end time")
        require_less_equal(start, end, "start time", "end time")
        require_less_equal(end, clip.length(), "start time", "end time")

        self.metrics = Metrics(self.metrics, length=end-start)

        self.start_frame = int(start * self.frame_rate())
        self.start_sample = int(start * self.sample_rate())

    def frame_signature(self, index):
        return self.clip.frame_signature(self.start_frame + index)

    def get_frame(self, index):
        return self.clip.get_frame(self.start_frame + index)

    def get_samples(self):
        original_samples = self.clip.get_samples()
        return original_samples[self.start_sample:self.start_sample+self.num_samples()]


class mono_to_stereo(MutatorClip):
    """ Change the number of channels from one to two. """
    def __init__(self, clip):
        super().__init__(clip)
        if self.clip.metrics.num_channels != 1:
            raise ValueError(f"Expected 1 audio channel, not {self.clip.num_channels()}.")
        self.metrics = Metrics(self.metrics, num_channels=2)
    def get_samples(self):
        data = self.clip.get_samples()
        return np.concatenate((data, data), axis=1)

class stereo_to_mono(MutatorClip):
    """ Change the number of channels from two to one. """
    def __init__(self, clip):
        super().__init__(clip)
        if self.clip.metrics.num_channels != 2:
            raise ValueError(f"Expected 2 audio channels, not {self.clip.num_channels()}.")
        self.metrics = Metrics(self.metrics, num_channels=1)
    def get_samples(self):
        data = self.clip.get_samples()
        return (0.5*data[:,0] + 0.5*data[:,1]).reshape(self.num_samples(), 1)

class reverse(MutatorClip):
    """ Reverse both the video and audio in a clip. """
    def frame_signature(self, index):
        return self.clip.frame_signature(self.num_frames() - index - 1)
    def get_frame(self, index):
        return self.clip.get_frame(self.num_frames() - index - 1)
    def get_samples(self):
        return np.flip(self.clip.get_samples(), axis=0)

class scale_volume(MutatorClip):
    """ Scale the volume of audio in a clip.  """
    def __init__(self, clip, factor):
        super().__init__(clip)
        require_float(factor, "scaling factor")
        require_positive(factor, "scaling factor")
        self.factor = factor

    def get_samples(self):
        return self.factor * self.clip.get_samples()

class crop(MutatorClip):
    """Trim the frames of a clip to show only the rectangle between
    lower_left and upper_right."""
    def __init__(self, clip, lower_left, upper_right):
        super().__init__(clip)
        require_int(lower_left[0], "lower left")
        require_non_negative(lower_left[0], "lower left")
        require_int(lower_left[1], "lower left")
        require_non_negative(lower_left[1], "lower left")
        require_int(upper_right[0], "upper right")
        require_less_equal(upper_right[0], clip.width(), "upper right", "width")
        require_int(upper_right[1], "upper right")
        require_less_equal(upper_right[1], clip.height(), "upper right", "width")
        require_less(lower_left[0], upper_right[0], "lower left", "upper right")
        require_less(lower_left[1], upper_right[1], "lower left", "upper right")

        self.lower_left = lower_left
        self.upper_right = upper_right

        self.metrics = Metrics(self.metrics,
                               width=upper_right[0]-lower_left[0],
                               height=upper_right[1]-lower_left[1])

    def frame_signature(self, index):
        return ['crop', self.lower_left, self.upper_right, self.clip.frame_signature(index)]

    def get_frame(self, index):
        frame = self.clip.get_frame(index)
        ll = self.lower_left
        ur = self.upper_right
        return frame[ll[1]:ur[1], ll[0]:ur[0], :]

class draw_text(VideoClip):
    """ A clip consisting of just a bit of text. """
    def __init__(self, text, font_filename, font_size, color, frame_rate, length):
        super().__init__()

        require_string(font_filename, "font filename")
        require_float(font_size, "font size")
        require_positive(font_size, "font size")
        require_color(color, "color")

        # Determine the size of the image we need.  Make sure the width and
        # height are odd, because Pillow won't create an image with an
        # odd-dimensioned size.
        draw = ImageDraw.Draw(Image.new("RGBA", (1,1)))
        font = get_font(font_filename, font_size)
        size = draw.textsize(text, font=font)

        self.metrics = Metrics(src=Clip.default_metrics,
                               width=size[0],
                               height=size[1],
                               frame_rate = frame_rate,
                               length=length)

        self.text = text
        self.font_filename = font_filename
        self.font_size = font_size
        self.color = color
        self.frame = None

    def frame_signature(self, index):
        return ['text', self.text, self.font_filename, self.font_size,
          self.frame_rate(), self.length()]

    def get_frame(self, index):
        if self.frame is None:
            # Use Pillow to draw the text.
            image = Image.new("RGBA", (self.width(), self.height()), (0,0,0,0))
            draw = ImageDraw.Draw(image)
            color = self.color
            draw.text((0,0,),
                      self.text,
                      font=get_font(self.font_filename, self.font_size),
                      fill=(color[2],color[1],color[0],255))
            frame = np.array(image)

            # Pillow seems not to handle transparency quite how one might
            # expect -- as far as I can tell, it seems to fill the entire text
            # rectangle with the target color, and then use the alpha channel
            # to "draw" the text.  These seemed to be resulting in rectanglar
            # blobs, instead of readable text in some cases.  (Hypothesis:
            # Sometimes the alpha channel is discarded at some point?)  Below,
            # we fix this by blending into a black background.
            bg = np.zeros(frame.shape, dtype=np.uint8)
            frame = alpha_blend(frame, bg)
            self.frame = frame

        return self.frame

class filter_frames(MutatorClip):
    """ A clip formed by passing the frames of another clip through some
    function.  The function can take either one or two arguments.  If it's one
    argument, that will be the frame itself.  If it's two arguments, it wil lbe
    the frame and its index.  In either case, it should return the output
    frame.  Output frames may have a different size from the input ones, but
    must all be the same size across the whole clip.  Audio remains unchanged.

    Optionally, provide an optional name to make the frame signatures more readable.

    Set size to None to infer the width and height of the result by executing
    the filter function.  Set size to a tuple (width, height) if you know them,
    to avoid generating a sample frame (which can be slow, for example, if that
    frame relies on a from_file clip).  Set size to "same" to assume the size
    is the same as the source clip.
    """

    def __init__(self, clip, func, name=None, size=None):
        super().__init__(clip)

        require_callable(func, "filter function")
        self.func = func

        # Use the details of the function's bytecode to generate a "signature",
        # which we'll use in the frame signatures.  This should help to prevent
        # the need to clear cache if the implementation of a filter function is
        # changed.
        bytecode = dis.Bytecode(func, first_line=0)
        description = bytecode.dis()
        self.sig = hashlib.sha1(description.encode('UTF-8')).hexdigest()[:7]


        # Acquire a name for the filter.
        if name is None:
            name = self.func.__name__
        self.name = name

        # Figure out if the function expects the index or not.  If not, wrap it
        # in a lambda to ignore the index.  But remember that we've done this,
        # so we can leave the index out of our frame signatures.
        parameters = list(inspect.signature(self.func).parameters)
        if len(parameters) == 1:
            self.depends_on_index = False
            def new_func(frame, index, func=self.func): #pylint: disable=unused-argument
                return func(frame)
            self.func = new_func
        elif len(parameters) == 2:
            self.depends_on_index = True
        else:
            raise TypeError(f"Filter function should accept either (frame) or "
                            f"(frame, index), not {parameters}.)")

        # Figure out the size.
        if size is None:
            sample_frame = self.func(clip.get_frame(0), 0)
            height, width, _ = sample_frame.shape
        else:
            try:
                width, height = size
                require_int(width, 'width')
                require_positive(width, 'width')
                require_int(height, 'height')
                require_positive(height, 'height')
            except ValueError as e:
                if size == "same":
                    width = clip.width()
                    height = clip.height()
                else:
                    raise ValueError(f'In filter_frames, did not understand size={size}.') from e

        self.metrics = Metrics(
          src = clip.metrics,
          width = width,
          height = height
        )


    def frame_signature(self, index):
        assert index < self.num_frames()
        return [f"{self.name} (filter:{self.sig})", {
          'index' : index if self.depends_on_index else None,
          'width' : self.width(),
          'height' : self.height(),
          'frame' : self.clip.frame_signature(index)
        }]

    def get_frame(self, index):
        return self.func(self.clip.get_frame(index), index)

def to_monochrome(clip):
    """ Convert a clip's video to monochrome. """
    def mono(frame):
        return cv2.cvtColor(cv2.cvtColor(frame, cv2.COLOR_BGRA2GRAY), cv2.COLOR_GRAY2BGRA)

    return filter_frames(
      clip=clip,
      func=mono,
      name='to_monochrome',
      size='same'
    )

def scale_to_size(clip, width, height):
    """Scale the frames of a clip to a given size, possibly distorting them."""
    require_clip(clip, "clip")
    require_int(width, "new width")
    require_positive(width, "new width")
    require_int(height, "new height")
    require_positive(height, "new height")

    def scale_filter(frame):
        return cv2.resize(frame, (width, height))

    return filter_frames(
      clip=clip,
      func=scale_filter,
      name=f'scale to {width}x{height}',
      size=(width,height)
    )

def scale_by_factor(clip, factor):
    """Scale the frames of a clip by a given factor."""
    require_clip(clip, "clip")
    require_float(factor, "scaling factor")
    require_positive(factor, "scaling factor")

    new_width = int(factor * clip.width())
    new_height = int(factor * clip.height())
    return scale_to_size(clip, new_width, new_height)

def scale_to_fit(clip, max_width, max_height):
    """Scale the frames of a clip to fit within the given constraints,
    maintaining the aspect ratio."""

    aspect1 = clip.width() / clip.height()
    aspect2 = max_width / max_height

    if aspect1 > aspect2:
        # Fill width.
        new_width = max_width
        new_height = clip.height() * max_width / clip.width()
    else:
        # Fill height.
        new_height = max_height
        new_width = clip.width() * max_height / clip.height()

    return scale_to_size(clip, int(new_width), int(new_height))

class static_frame(VideoClip):
    """ Show a single image over and over, silently. """
    def __init__(self, the_frame, frame_name, frame_rate, length):
        super().__init__()
        try:
            height, width, depth = the_frame.shape
        except AttributeError as e:
            raise TypeError(f"Cannot not get shape of {the_frame}.") from e
        except ValueError as e:
            raise ValueError(f"Could not get width, height, and depth of {the_frame}."
              f" Shape is {the_frame.shape}.") from e
        if depth != 4:
            raise ValueError(f"Frame {the_frame} does not have 4 channels."
              f" Shape is {the_frame.shape}.")

        self.metrics = Metrics(src=Clip.default_metrics,
                               width=width,
                               height=height,
                               frame_rate = frame_rate,
                               length=length)

        self.the_frame = the_frame.copy()
        hash_source = self.the_frame.tobytes()
        self.sig = hashlib.sha1(hash_source).hexdigest()[:7]

        self.frame_name = frame_name

    def frame_signature(self, index):
        return [ 'static_frame', {
          'name': self.frame_name,
          'sig': self.sig
        }]

    def get_frame(self, index):
        return self.the_frame

def static_image(filename, frame_rate, length):
    """ Show a single image loaded from a file over and over, silently. """
    the_frame = read_image(filename)
    assert the_frame is not None
    return static_frame(the_frame, filename, frame_rate, length)


class resample(MutatorClip):
    """ Change some combination of the frame rate, sample rate, and length. """
    def __init__(self, clip, frame_rate=None, sample_rate=None, length=None):

        super().__init__(clip)

        if frame_rate is not None:
            require_float(frame_rate, "frame rate")
            require_positive(frame_rate, "frame rate")
        else:
            frame_rate = self.clip.frame_rate()

        if sample_rate is not None:
            require_float(sample_rate, "sample rate")
            require_positive(sample_rate, "sample rate")
        else:
            sample_rate = self.clip.sample_rate()

        if length is not None:
            require_float(length, "length")
            require_positive(length, "length")
        else:
            length = self.clip.length()

        self.metrics = Metrics(src=self.clip.metrics,
                               frame_rate=frame_rate,
                               sample_rate=sample_rate,
                               length=length)

    def new_index(self, index):
        """ Return the index in the original clip to be used at the given index
        of the present clip. """
        # print()
        # print("index:", index)
        # print("self.length():", self.length())
        # print("self.num_frames():", self.num_frames())
        assert index < self.num_frames()
        seconds_here = self.length() * index / self.num_frames()
        # print("seconds_here:", seconds_here)
        assert seconds_here <= self.length()
        seconds_there = seconds_here * self.clip.length() / self.length()
        # print("seconds_there:", seconds_there)
        # print("self.clip.frame_rate():", self.clip.frame_rate())
        assert seconds_there <= self.clip.length()
        index_there = int(seconds_there * self.clip.frame_rate())
        # print("index_there:", index_there)
        # print("correct time range for index_there:",
        #      index_there/self.clip.frame_rate(), (index_there+1)/self.clip.frame_rate())
        # print("self.clip.num_frames():", self.clip.num_frames())
        assert index_there < self.clip.num_frames(), f'{index_there}, {self.clip.num_frames()}'
        return index_there

    def frame_signature(self, index):
        return self.clip.frame_signature(self.new_index(index))

    def get_frame(self, index):
        return self.clip.get_frame(self.new_index(index))

    def get_samples(self):
        data = self.clip.get_samples()
        if self.clip.sample_rate() != self.sample_rate() or self.clip.length() != self.length():
            data = scipy.signal.resample(data, self.num_samples())
        return data

class fade_base(MutatorClip, ABC):
    """Fade in from or out to silent black or silent transparency."""

    def __init__(self, clip, fade_length, transparent=False):
        super().__init__(clip)
        require_float(fade_length, "fade length")
        require_non_negative(fade_length, "fade length")
        require_less_equal(fade_length, clip.length(), "fade length", "clip length")
        self.fade_length = fade_length
        self.transparent = transparent

    @abstractmethod
    def alpha(self, index):
        """ At the given index, what scaling factor should we apply?"""

    def frame_signature(self, index):
        sig = self.clip.frame_signature(index)
        alpha = self.alpha(index)
        if alpha == 1.0:
            return sig
        elif self.transparent:
            return [f'faded to transparent by {alpha}', sig]
        else:
            return [f'faded to black by {alpha}', sig]

    def get_frame(self, index):
        alpha = self.alpha(index)
        assert alpha >= 0.0
        assert alpha <= 1.0
        frame = self.clip.get_frame(index).copy()
        if alpha == 1.0:
            return frame
        elif self.transparent:
            frame[:,:,3] = (frame[:,:,3]*alpha).astype(np.uint8)
            return frame.astype(np.uint8)
        else:
            return (alpha * frame).astype(np.uint8)

    @abstractmethod
    def get_samples(self):
        """ Return samples; implemented in fade_in and fade_out below."""

class fade_in(fade_base):
    """ Fade in from silent black or silent transparency. """
    def alpha(self, index):
        return min(1, index/int(self.fade_length * self.clip.frame_rate()))
    def get_samples(self):
        a = self.clip.get_samples().copy()
        length = int(self.fade_length * self.sample_rate())
        num_channels = self.num_channels()
        a[0:length] *= np.linspace([0.0]*num_channels, [1.0]*num_channels, length)
        return a

class fade_out(fade_base):
    """ Fade out to silent black or silent transparency. """
    def alpha(self, index):
        return min(1, (self.clip.num_frames() - index)
                       / int(self.fade_length * self.clip.frame_rate()))

    def get_samples(self):
        a = self.clip.get_samples().copy()
        length = int(self.fade_length * self.sample_rate())
        num_channels = self.num_channels()
        a[a.shape[0]-length:a.shape[0]] *= np.linspace([1.0]*num_channels,
                                                       [0.0]*num_channels, length)
        return a

def slice_out(clip, start, end):
    """ Remove the part between the given endponts. """
    require_clip(clip, "clip")
    require_float(start, "start time")
    require_non_negative(start, "start time")
    require_float(end, "end time")
    require_positive(end, "end time")
    require_less(start, end, "start time", "end time")
    require_less_equal(end, clip.length(), "end time", "clip length")

    return chain(slice_clip(clip, 0, start),
                 slice_clip(clip, end, clip.length()))


def letterbox(clip, width, height):
    """ Fix the clip within given dimensions, adding black bands on the
    top/bottom or left/right if needed. """
    require_clip(clip, "clip")
    require_int(width, "width")
    require_positive(width, "width")
    require_int(height, "height")
    require_positive(height, "height")

    scaled = scale_to_fit(clip, width, height)

    position=[int((width-scaled.width())/2),
              int((height-scaled.height())/2)]

    return composite(Element(clip=scaled,
                             start_time=0,
                             position=position,
                             video_mode=VideoMode.REPLACE,
                             audio_mode=AudioMode.REPLACE),
                      width=width,
                      height=height)


class repeat_frame(VideoClip):
    """Shows the same frame, from another clip, over and over."""
    def __init__(self, clip, when, length):
        super().__init__()
        require_clip(clip, "clip")
        require_float(when, "time")
        require_non_negative(when, "time")
        require_less_equal(when, clip.length(), "time", "clip length")
        require_float(length, "length")
        require_positive(length, "length")

        self.metrics = Metrics(src=clip.metrics,
                               length=length)
        self.clip = clip
        self.frame_index = int(when * self.frame_rate())

        # Special case for repeating at/near the end of the clip: Because of
        # how time is scaled, we might end up asking for one frame beyond the
        # end.
        if self.frame_index == self.clip.num_frames():
            self.frame_index -= 1

        assert self.frame_index < self.clip.num_frames()

    def frame_signature(self, index):
        assert index < self.num_frames()
        return self.clip.frame_signature(self.frame_index)

    def get_frame(self, index):
        return self.clip.get_frame(self.frame_index)

def hold_at_end(clip, target_length):
    """Extend a clip by repeating its last frame, to fill a target length."""
    require_clip(clip, "clip")
    require_float(target_length, "target length")
    require_positive(target_length, "target length")

    # Here the repeat_frame almost certainly goes beyond target length, and
    # we force the final product to have the right length directly.  This
    # prevents getting a blank frame at end in some cases.
    return chain(clip,
                 repeat_frame(clip, clip.length(), target_length),
                 length=target_length)


class image_glob(VideoClip):
    """Video from a collection of identically-sized image files that match
    a unix-style pattern, at a given frame rate."""
    def __init__(self, pattern, frame_rate):
        super().__init__()

        require_string(pattern, "pattern")
        require_float(frame_rate, "frame rate")
        require_positive(frame_rate, "frame rate")

        self.pattern = pattern

        self.filenames = sorted(glob.glob(pattern))
        if len(self.filenames) == 0:
            raise FileNotFoundError(f'No files matched pattern: {pattern}')

        # Get full pathnames, in case the current directory changes.
        self.filenames = list(map(lambda x: os.path.join(os.getcwd(), x), self.filenames))

        sample_frame = cv2.imread(self.filenames[0])
        assert sample_frame is not None

        self.metrics = Metrics(src=Clip.default_metrics,
                               width=sample_frame.shape[1],
                               height=sample_frame.shape[0],
                               frame_rate = frame_rate,
                               length = len(self.filenames)/frame_rate)

    def frame_signature(self, index):
        assert index<self.num_frames()
        return self.filenames[index]

    def get_frame(self, index):
        return read_image(self.filenames[index])

class zip_file(VideoClip):
    """ A video clip from images stored in a zip file."""

    def __init__(self, fname, frame_rate):
        super().__init__()

        require_string(fname, "file name")
        require_float(frame_rate, "frame rate")
        require_positive(frame_rate, "frame rate")

        if not os.path.isfile(fname):
            raise FileNotFoundError(f'Trying to open {fname}, which does not exist.')

        self.fname = fname
        self.zf = zipfile.ZipFile(fname, 'r') #pylint: disable=consider-using-with

        image_formats = ['tga', 'jpg', 'jpeg', 'png'] # (Note: Many others could be added here.)
        pattern = ".(" + "|".join(image_formats) + ")$"

        info_list = self.zf.infolist()
        info_list = filter(lambda x: re.search(pattern, x.filename), info_list)
        info_list = sorted(info_list, key=lambda x: x.filename)
        self.info_list = info_list

        self.frame_rate_ = frame_rate

        sample_frame = self.get_frame(0)

        self.metrics = Metrics(src = Clip.default_metrics,
                               width=sample_frame.shape[1],
                               height=sample_frame.shape[0],
                               frame_rate = frame_rate,
                               length = len(self.info_list)/frame_rate)

    def frame_signature(self, index):
        return ['zip file member', self.fname, self.info_list[index].filename]

    def get_frame(self, index):
        data = self.zf.read(self.info_list[index])
        pil_image = Image.open(io.BytesIO(data)).convert('RGBA')
        frame = np.array(pil_image)
        frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGRA)
        return frame

def to_default_metrics(clip):
    """Adjust a clip so that its metrics match the default metrics: Scale video
    and resample to match frame rate and sample rate.  Useful if assorted clips
    from various sources will be chained together."""

    require_clip(clip, "clip")

    dm = Clip.default_metrics

    # Video dimensions.
    if clip.width() != dm.width or clip.height() != dm.height:
        clip = letterbox(clip, dm.width, dm.height)

    # Frame rate and sample rate.
    if (clip.frame_rate() != dm.frame_rate
          or clip.sample_rate() != dm.sample_rate):
        clip = resample(clip,
                        frame_rate=dm.frame_rate,
                        sample_rate=dm.sample_rate)

    # Number of audio channels.
    nc_before = clip.num_channels()
    nc_after = dm.num_channels
    if nc_before == nc_after:
        pass
    elif nc_before == 2 and nc_after == 1:
        clip = stereo_to_mono(clip)
    elif nc_before == 1 and nc_after == 2:
        clip = mono_to_stereo(clip)
    else:
        raise NotImplementedError(f"Don't know how to convert from {nc_before}"
                                  f"channels to {nc_after}.")

    return clip

def timewarp(clip, factor):
    """ Speed up a clip by the given factor. """
    require_clip(clip, "clip")
    require_float(factor, "factor")
    require_positive(factor, "factor")

    return resample(clip, length=clip.length()/factor)

def pdf_page(pdf_file, page_num, frame_rate, length, **kwargs):
    """A silent video constructed from a single page of a PDF."""
    require_string(pdf_file, "file name")
    require_int(page_num, "page number")
    require_positive(page_num, "page number")
    require_float(frame_rate, "frame rate")
    require_positive(frame_rate, "frame rate")
    require_float(length, "length")
    require_positive(length, "length")

    # Hash the file.  We'll use this in the name of the static_frame below, so
    # that things are re-generated correctly when the PDF changes.
    pdf_hash = sha256sum_file(pdf_file)

    # Get an image of the PDF.
    images = pdf2image.convert_from_path(pdf_file,
                                         first_page=page_num,
                                         last_page=page_num,
                                         **kwargs)
    image = images[0].convert('RGBA')
    frame = np.array(image)
    frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2BGRA)

    # Sometimes we get, for reasons not adequately understood, an image that is
    # not the correct size, off by one in the width.  Fix it.
    if 'size' in kwargs:
        w = kwargs['size'][0]
        h = kwargs['size'][1]
        if h != frame.shape[0] or w != frame.shape[1]:
            frame = frame[0:h,0:w]  # pragma: no cover

    # Form a clip that shows this image repeatedly.
    return static_frame(frame,
                        frame_name=f'{pdf_file} ({pdf_hash}), page {page_num} {kwargs}',
                        frame_rate=frame_rate,
                        length=length)

class spin(MutatorClip):
    """ Rotate the contents of a clip about the center, a given number of
    times. Rotational velocity is computed to complete the requested rotations
    within the length of the original clip."""
    def __init__(self, clip, total_rotations):
        super().__init__(clip)

        require_float(total_rotations, "total rotations")
        require_non_negative(total_rotations, "total rotations")

        # Leave enough space to show the full undrlying clip at every
        # orientation.
        self.radius = math.ceil(math.sqrt(clip.width()**2 + clip.height()**2))

        self.metrics = Metrics(src=clip.metrics,
                               width=self.radius,
                               height=self.radius)

        # Figure out how much to rotate in each frame.
        rotations_per_second = total_rotations / clip.length()
        rotations_per_frame = rotations_per_second / clip.frame_rate()
        self.degrees_per_frame = 360 * rotations_per_frame

    def frame_signature(self, index):
        assert index < self.num_frames()
        sig = self.clip.frame_signature(index)
        degrees = self.degrees_per_frame * index
        return [f'rotated by {degrees}', sig]

    def get_frame(self, index):
        frame = np.zeros([self.radius, self.radius, 4], np.uint8)
        original_frame = self.clip.get_frame(index)

        a = (frame.shape[0] - original_frame.shape[0])
        b = (frame.shape[1] - original_frame.shape[1])

        frame[
            int(a/2):int(a/2)+original_frame.shape[0],
            int(b/2):int(b/2)+original_frame.shape[1],
            :
        ] = original_frame

        degrees = self.degrees_per_frame * index

        # https://stackoverflow.com/questions/9041681/opencv-python-rotate-image-by-x-degrees-around-specific-point
        image_center = tuple(np.array(frame.shape[1::-1]) / 2)
        rot_mat = cv2.getRotationMatrix2D(image_center, degrees, 1.0)
        rotated_frame = cv2.warpAffine(frame,
                                       rot_mat,
                                       frame.shape[1::-1],
                                       flags=cv2.INTER_NEAREST,
                                       borderMode=cv2.BORDER_CONSTANT,
                                       borderValue=[0,0,0,0])
        # Using INTER_NEAREST here instead of INTER_LINEAR has two effects:
        # 1. It prevents an artifical "border" from appearing when INTER_LINEAR
        # blends "real" pixels with the background zeros around the edge of the
        # real image.  This is sort of built in if we rotate when there are
        # "real" pixels close to [0,0,0,0] background pixels.
        # 2. It gives straight lines a jagged looks.
        #
        # Perhaps a better version might someday get the best of both worlds by
        # embedding the real image in a larger canvas (filled somehow with the
        # right color -- perhaps by grabbing from the boundary of the real
        # image?), rotating that larger image with INTER_LINEAR (creating an
        # ugly by distant border), and then cropping back to the radius x
        # radius size that we need.

        return rotated_frame

class Align(Enum):
    """ When stacking clips, how should each be placed? """
    CENTER = 1
    LEFT = 2
    TOP = 3
    START = 4
    RIGHT = 5
    BOTTOM = 6
    END = 7

def stack_clips(*args, align, min_dim=0, vert, name):
    """ Arrange a series of clips in a stack, either vertically or
    horizontally.  Probably use vstack or hstack to call this. """

    # Flatten things out, in case the inputs were wrapped in a list.
    clips = flatten_args(args)

    # Compute the width or height.  Do this first so we can maybe center things
    # below.
    dim = min_dim
    for clip in clips:
        if isinstance(clip, Clip):
            clip_dim = clip.width() if vert else clip.height()
            dim = max(dim, clip_dim)
        elif is_int(clip):
            pass
        else:
            raise TypeError(f"In {name}, got a {type(clip)} instead of Clip or int.")

    # Sanity check the alignment.
    if vert:
        valid_aligns = [Align.LEFT, Align.RIGHT, Align.CENTER]
    else:
        valid_aligns = [Align.TOP, Align.BOTTOM, Align.CENTER]

    if align not in valid_aligns:
        raise NotImplementedError(f"Don't know how to align {align} in {name}.")


    # Place each clip in the composite in the correct place.

    a = 0  # The coordinate that we compute each time based on align.
    b = 0  # The coordinate that moves steady forward.

    elements = []

    for clip in clips:
        if isinstance(clip, Clip):
            clip_dim = clip.width() if vert else clip.height()

            if align in [Align.LEFT, Align.TOP]:
                a = 0
            elif align==Align.CENTER:
                a = int((dim - clip_dim)/2)
            else: # align in [Align.RIGHT, Align.BOTTOM]
                a = dim - clip_dim

            elements.append(Element(clip=clip,
                                    start_time=0,
                                    position=[a, b] if vert else [b, a]))

            b += clip.height() if vert else clip.width()
        else: # must be an int, as checked above
            b += clip

    if vert:
        return composite(elements, width=dim, height=b)
    else:
        return composite(elements, height=dim, width=b)

def vstack(*args, align=Align.CENTER, min_width=0):
    """ Arrange a series of clips in a vertical stack. """
    return stack_clips(args, align=align, min_dim=min_width, vert=True, name='vstack')

def hstack(*args, align=Align.CENTER, min_height=0):
    """ Arrange a series of clips in a horizontal row. """
    return stack_clips(args, align=align, min_dim=min_height, vert=False, name='hstack')

def background(clip, bg_color):
    """ Blend a clip onto a same-sized background of the given color. """
    require_clip(clip, 'clip')
    require_color(bg_color, 'background color')

    return composite(Element(solid(bg_color,
                                   clip.width(),
                                   clip.height(),
                                   clip.frame_rate(),
                                   clip.length()),
                             0,
                             (0,0)),
                      Element(clip,
                              0,
                              (0,0),
                              video_mode=VideoMode.BLEND))


def superimpose_center(under_clip, over_clip, start_time, audio_mode=AudioMode.ADD):
    """Superimpose one clip on another, in the center of each frame, starting at
    a given time."""
    require_clip(under_clip, "under clip")
    require_clip(over_clip, "over clip")
    require_float(start_time, "start time")
    require_non_negative(start_time, "start time")

    x = int(under_clip.width()/2) - int(over_clip.width()/2)
    y = int(under_clip.height()/2) - int(over_clip.height()/2)

    return composite(Element(under_clip, 0, [0,0], VideoMode.REPLACE),
                     Element(over_clip, start_time, [x,y], VideoMode.REPLACE, audio_mode))

def loop(clip, length):
    """Repeat a clip as needed to fill the given length."""
    require_clip(clip, "clip")
    require_float(length, "length")
    require_positive(length, "length")

    full_plays = int(length/clip.length())
    partial_play = length - full_plays*clip.length()
    return chain(full_plays*[clip], slice_clip(clip, 0, partial_play))

class ken_burns(MutatorClip):
    """ Pan and/or zoom through a clip over time. """
    def __init__(self, clip, width, height, start_top_left, start_bottom_right,
                 end_top_left, end_bottom_right):
        super().__init__(clip)

        # So. Many. Ways to mess up.
        require_int_point(start_top_left, "start top left")
        require_int_point(start_bottom_right, "start bottom right")
        require_int_point(end_top_left, "end top left")
        require_int_point(end_bottom_right, "end bottom right")
        require_non_negative(start_top_left[0], "start top left x")
        require_non_negative(start_top_left[1], "start top left y")
        require_non_negative(end_top_left[0], "end top left x")
        require_non_negative(end_top_left[1], "end top left y")
        require_less(start_top_left[0], start_bottom_right[0],
                     "start top left x", "start bottom right x")
        require_less(start_top_left[1], start_bottom_right[1],
                     "start top left y", "start bottom right y")
        require_less(end_top_left[0], end_bottom_right[0],
                     "end top left x", "end bottom right x")
        require_less(end_top_left[1], end_bottom_right[1],
                     "end top left y", "end bottom right y")
        require_less_equal(start_bottom_right[0], clip.width(),
                           "start bottom right x", "clip width")
        require_less_equal(start_bottom_right[1], clip.height(),
                           "start bottom right y", "clip height")
        require_less_equal(end_bottom_right[0], clip.width(),
                           "end bottom right x", "clip width")
        require_less_equal(end_bottom_right[1], clip.height(),
                           "end bottom right y", "clip height")


        start_ratio = ((start_bottom_right[0] - start_top_left[0])
                       / (start_bottom_right[1] - start_top_left[1]))

        end_ratio = ((end_bottom_right[0] - end_top_left[0])
                     / (end_bottom_right[1] - end_top_left[1]))

        output_ratio = width/height

        if not math.isclose(start_ratio, output_ratio, abs_tol=0.01):
            raise ValueError("This ken_burns effect will distort the image at the start. "
                             f'Starting aspect ratio is {start_ratio}. '
                             f'Output aspect ratio is {output_ratio}. ')

        if not math.isclose(end_ratio, output_ratio, abs_tol=0.01):
            raise ValueError("This ken_burns effect will distort the image at the end. "
                             f'Ending aspect ratio is {end_ratio}. '
                             f'Output aspect ratio is {output_ratio}. ')

        self.start_top_left = np.array(start_top_left)
        self.start_bottom_right = np.array(start_bottom_right)
        self.end_top_left = np.array(end_top_left)
        self.end_bottom_right = np.array(end_bottom_right)

        self.metrics = Metrics(src=clip.metrics,
                               width=width,
                               height=height)

    def get_corners(self, index):
        """ Return the top left and bottom right corners of the view at the
        given frame index. """
        alpha = index/self.clip.num_frames()
        p1 = (((1-alpha)*self.start_top_left + alpha*self.end_top_left))
        p2 = (((1-alpha)*self.start_bottom_right + alpha*self.end_bottom_right))
        p1 = np.around(p1).astype(int)
        p2 = np.around(p2).astype(int)
        return p1, p2

    def frame_signature(self, index):
        p1, p2 = self.get_corners(index)
        return ['ken_burns', {'top_left': p1,
                              'bottom_right': p2,
                              'frame':self.clip.frame_signature(index)}]

    def get_frame(self, index):
        p1, p2 = self.get_corners(index)
        frame = self.clip.get_frame(index)
        fragment = frame[p1[1]:p2[1],p1[0]:p2[0],:]
        sized_fragment = cv2.resize(fragment, (self.width(), self.height()))
        return sized_fragment


def fade_between(clip1, clip2):
    """ Fade from one clip to another.  Both must have the same length. """
    require_clip(clip1, "first clip")
    require_clip(clip2, "second clip")
    require_equal(clip1.length(), clip2.length(), "clip lengths")

    return chain(clip1, clip2, fade_time=clip1.length())

class silence_audio(MutatorClip):
    """ Replace whatever audio we have with silence. """
    def get_samples(self):
        return np.zeros([self.metrics.num_samples(), self.metrics.num_channels])

